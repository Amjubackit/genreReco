{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "IMPORT SECTION"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import ast\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import csv\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.width', None)\n",
    "import nltk\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('cmudict')\n",
    "# nltk.download('vader_lexicon')\n",
    "\n",
    "from nltk.corpus import cmudict\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "\n",
    "with open('creds.json', 'r') as fp:\n",
    "    creds_dict = json.load(fp)\n",
    "spotify = spotipy.Spotify(\n",
    "    client_credentials_manager=SpotifyClientCredentials(**creds_dict)\n",
    ")\n",
    "\n",
    "GENRE_LIST = [\n",
    "    'pop', 'rock', 'hip-hop', 'rap', 'r&b', 'soul', 'electronic', 'dance', 'country', 'jazz', 'classical',\n",
    "    'reggae', 'alternative', 'indie', 'folk', 'metal', 'punk', 'blues', 'world', 'funk', 'disco', 'gospel'\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "GENERAL UTILS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def fix_genre(genre_string):\n",
    "    mapping = {'hip hop': 'hip-hop', 'hip pop': 'hip-hop', \"metalcore\" : \"metal\"}\n",
    "    if genre_string.lower() not in mapping.keys():\n",
    "        return genre_string\n",
    "\n",
    "    return mapping.get(genre_string)\n",
    "\n",
    "\n",
    "def split_list_items(list_items):\n",
    "    merged_list = [item.split() for item in list_items]\n",
    "    flattened_list = [fix_genre(word) for sublist in merged_list for word in sublist]\n",
    "    return flattened_list\n",
    "\n",
    "\n",
    "def remove_non_genres(word_list):\n",
    "    return [item for item in word_list if item in GENRE_LIST]\n",
    "\n",
    "\n",
    "def get_common_genre(song_genres):\n",
    "    translated = [fix_genre(item) for item in song_genres]\n",
    "    new_list = split_list_items(translated)\n",
    "    clean_list = remove_non_genres(new_list)\n",
    "    genre_counts = Counter(clean_list)\n",
    "    most_common_genre = genre_counts.most_common(1)\n",
    "    return most_common_genre[0][0]\n",
    "\n",
    "def purify_text(text):\n",
    "    # Make replace table\n",
    "    trans_table = str.maketrans({'&': 'and', 'Ã©': 'e'})\n",
    "    text = text.translate(trans_table)\n",
    "    # Remove anything inside parentheses (including parentheses)\n",
    "    text = re.sub(r'\\([^)]*\\)', '', text)\n",
    "    # Remove special characters\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    # Remove extra spaces\n",
    "    text = ' '.join(text.split())\n",
    "    # Replace spaces with dash\n",
    "    text = re.sub(r'\\s', '-', text)\n",
    "\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "def save_lyrics(txt, file_path):\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(txt)\n",
    "\n",
    "\n",
    "def remove_file(file_path):\n",
    "    try:\n",
    "        os.remove(file_path)\n",
    "        print(\"File removed successfully\")\n",
    "    except OSError as e:\n",
    "        pass\n",
    "\n",
    "\n",
    "def get_file_extension(filepath):\n",
    "    # Get the file extension from the file path\n",
    "    _, file_extension = os.path.splitext(filepath)\n",
    "\n",
    "    # Check if the file extension is supported\n",
    "    if file_extension not in ['.json', '.csv']:\n",
    "        raise ValueError(\"Unsupported file format. Only 'json' and 'csv' formats are supported.\")\n",
    "\n",
    "    return file_extension\n",
    "\n",
    "\n",
    "def save_dataset(data, filepath, overwrite=False):\n",
    "    file_extension = get_file_extension(filepath)\n",
    "\n",
    "    # Check if the file already exists and overwrite is not enabled\n",
    "    if os.path.exists(filepath) and not overwrite:\n",
    "        raise FileExistsError(\"File already exists. Set 'overwrite' to True to overwrite the file.\")\n",
    "\n",
    "    # Save data as JSON\n",
    "    if file_extension == '.json':\n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            data.to_json(filepath, orient='records')\n",
    "\n",
    "        else:\n",
    "            with open(filepath, 'w') as file:\n",
    "                json.dump(data, file)\n",
    "\n",
    "    # Save data as CSV\n",
    "    elif file_extension == '.csv':\n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            data.to_csv(filepath, index=False)\n",
    "\n",
    "        else:\n",
    "            with open(filepath, 'w', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                # Assuming the dictionary has consistent keys across all entries\n",
    "                writer.writerow(data[0].keys())  # Write header\n",
    "                writer.writerows([entry.values() for entry in data])\n",
    "\n",
    "    print(f\"Data successfully saved as '{filepath}'.\")\n",
    "\n",
    "\n",
    "def load_dataset(filepath):\n",
    "    file_extension = get_file_extension(filepath)\n",
    "    # Load data from JSON\n",
    "    if file_extension == '.json':\n",
    "        with open(filepath, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        return pd.DataFrame(data)\n",
    "\n",
    "    # Load data from CSV\n",
    "    if file_extension == '.csv':\n",
    "        return pd.read_csv(filepath)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "SPOTIFY UTILS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def fetch_tracks(search_query=None, limit=10, offset=0):\n",
    "    if search_query is None:\n",
    "        search_query = \"\"\n",
    "    print(f\"SEARCH QUERY: {search_query}\")\n",
    "    response = spotify.search(search_query, limit=limit, offset=offset, type=\"track\")\n",
    "    return response.get(\"tracks\", {}).get(\"items\", [])\n",
    "\n",
    "\n",
    "def fetch_track_data(track_obj):\n",
    "    track_data = extract_tracks_data(track_obj, audio_features=False)\n",
    "    track_data[\"source_genre\"] = genre\n",
    "    lyrics_file = os.path.join(\"song_lyrics\", f\"{purify_text(track_data.get('artists'))}-{purify_text(track_data.get('name'))}.txt\")\n",
    "\n",
    "    # RENAME FILES FOR CONSISTENCY\n",
    "    old_file_path = os.path.join(\"song_lyrics\", f\"{track_data.get('artists')}-{track_data.get('name')}.txt\")\n",
    "    if is_file_valid(old_file_path):\n",
    "        os.rename(old_file_path, lyrics_file)\n",
    "\n",
    "\n",
    "    if not is_file_valid(lyrics_file):\n",
    "        print(\"LYRICS FILE IS NOT FOUND OR NOT VALID\")\n",
    "        # CRAWL GENIUS TO EXTRACT LYRICS TEXT FILE\n",
    "        lyrics = fetch_lyrics(track_data.get('artists'), track_data.get('name'))\n",
    "        lyrics_to_save = ''.join(lyrics) if lyrics else \"\"\n",
    "        save_lyrics(lyrics_to_save, lyrics_file)\n",
    "\n",
    "    lyrics_text = load_song_lyrics(lyrics_file)\n",
    "    if lyrics_text:\n",
    "        # EXTRACT LYRICS FEATURES\n",
    "        lyrics_features = extract_lyrics_features(lyrics_text)\n",
    "        track_data.update(lyrics_features)\n",
    "    else:\n",
    "        pass\n",
    "        # print(\"FAILED TO LOAD SONG LYRICS, NOT SAVING\")\n",
    "        # TODO: dont save (continue) if lyrics not found\n",
    "\n",
    "    return track_data\n",
    "\n",
    "\n",
    "def extract_tracks_data(track, audio_features=False):\n",
    "    print(f\"EXTRACTING TRACK FEATURES, AUDIO FEATURES = {audio_features}\")\n",
    "    artists = track.get(\"artists\")[0]\n",
    "    artist_id = artists.get(\"id\")\n",
    "    artist_info = spotify.artist(artist_id)\n",
    "    artist_genres = artist_info.get(\"genres\", [])\n",
    "\n",
    "    track_dict = {\n",
    "        \"name\": str(track.get(\"name\")),\n",
    "        \"artists\": str(artist_info.get(\"name\")),\n",
    "        \"release_date\": track.get(\"album\", {}).get(\"release_date\"),\n",
    "        \"genres\": list(artist_genres),\n",
    "        \"common_genre\": get_common_genre(artist_genres),\n",
    "        \"duration\": int(track.get(\"duration_ms\")),\n",
    "        \"popularity\": int(track.get(\"popularity\"))\n",
    "    }\n",
    "\n",
    "    if audio_features:\n",
    "        track_audio_features = spotify.audio_features(track.get(\"uri\"))[0]\n",
    "        track_dict.update(**{key: float(track_audio_features.get(key)) for key in\n",
    "           [\"danceability\", \"energy\", \"key\", \"loudness\", \"speechiness\", \"acousticness\", \"instrumentalness\",\n",
    "            \"liveness\", \"valence\", \"tempo\"]})\n",
    "\n",
    "    return track_dict\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "GENIUS CRAWLING UTILS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def fetch_lyrics(artist, song_title):\n",
    "    # Format the artist and song title for the URL\n",
    "    print(f\"\\nARTIST: {artist}\")\n",
    "    print(f\"SONG TITLE: {song_title}\")\n",
    "    artist = purify_text(artist)\n",
    "    song_title = purify_text(song_title)\n",
    "    url = f\"https://genius.com/{artist}-{song_title}-lyrics\"\n",
    "\n",
    "    print(f\"Fetching: {url}\")\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        print(\"I got something\")\n",
    "        # Use BeautifulSoup to parse the HTML content\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Find the lyrics section\n",
    "        lyrics_divs = soup.find_all('div', attrs={\"data-lyrics-container\": \"true\"})\n",
    "\n",
    "        lyrics_lines = [d.get_text('\\n') for d in lyrics_divs]\n",
    "\n",
    "        return lyrics_lines\n",
    "\n",
    "    print(f\"Lyrics data not found\")\n",
    "    # Return None if lyrics couldn't be fetched\n",
    "    return None"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "LYRICS FEATURES UTILS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def vers_chorus_count(song_lines):\n",
    "    vers_count = 0\n",
    "    chorus_count = 0\n",
    "    for line in song_lines:\n",
    "        line = line.strip()\n",
    "        if line.startswith(\"[Chorus\"):\n",
    "            chorus_count += 1\n",
    "        elif line.startswith(\"[Verse\"):\n",
    "            vers_count += 1\n",
    "\n",
    "    return vers_count, chorus_count\n",
    "\n",
    "def count_stop_words(lyrics):\n",
    "    stopwords_list = set(stopwords.words('english'))\n",
    "    return sum(1 for word in word_tokenize(lyrics) if word.lower() in stopwords_list)\n",
    "\n",
    "\n",
    "def load_song_lyrics(text_file):\n",
    "    with open(text_file, 'r') as tf:\n",
    "        lyrics = tf.read()\n",
    "\n",
    "    return lyrics\n",
    "\n",
    "\n",
    "def count_words(lyrics, unique=False):\n",
    "    return len(lyrics.split(\" \")) if not unique else len(set(lyrics.split(\" \")))\n",
    "\n",
    "\n",
    "def average_word_length(lyrics):\n",
    "    words = lyrics.split(\" \")\n",
    "    total_length = sum(len(word) for word in words)\n",
    "    return total_length / len(words) if len(words) > 0 else 0\n",
    "\n",
    "\n",
    "def sentiment_analysis(lyrics):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    sentiment = sid.polarity_scores(lyrics)\n",
    "    return sentiment\n",
    "\n",
    "\n",
    "def find_rhymes_count(lyrics):\n",
    "    pronunciations = cmudict.dict()\n",
    "    words = word_tokenize(lyrics.lower())\n",
    "    unique_words = set(words)\n",
    "\n",
    "    rhymes_count = 0\n",
    "    for word in unique_words:\n",
    "        try:\n",
    "            phonemes = pronunciations[word]\n",
    "        except KeyError:\n",
    "            continue\n",
    "        rhyming_words = []\n",
    "        for rhyme_word, rhyme_phonemes in pronunciations.items():\n",
    "            if rhyme_word != word and rhyme_phonemes[-2:] == phonemes[-2:]:\n",
    "                rhyming_words.append(rhyme_word)\n",
    "        rhymes_count += len(rhyming_words)\n",
    "\n",
    "    return rhymes_count\n",
    "\n",
    "\n",
    "def extract_lyrics_features(song_lyrics):\n",
    "\n",
    "    lines = song_lyrics.split('\\n')\n",
    "    lines_count = len(lines)\n",
    "    lyrics_no_new_lines = ' '.join(lines)\n",
    "    word_count = count_words(lyrics_no_new_lines)\n",
    "    unique_words = count_words(lyrics_no_new_lines, unique=True)\n",
    "    avg_word_length = average_word_length(lyrics_no_new_lines)\n",
    "    stopwords_count = count_stop_words(lyrics_no_new_lines)\n",
    "    vers_cnt, chorus_cnt = vers_chorus_count(lines)\n",
    "    sentiment = sentiment_analysis(lyrics_no_new_lines)\n",
    "    # rhymes = find_rhymes_count(song_lyrics)\n",
    "    # rhymes_perc = round(rhymes / word_count, 2)\n",
    "\n",
    "    features = {\n",
    "        \"lines_count\": lines_count,\n",
    "        \"word_count\": word_count,\n",
    "        \"unique_words\": unique_words,\n",
    "        \"stopwords_count\":stopwords_count,\n",
    "        \"avg_word_length\": avg_word_length,\n",
    "        \"chorus_count\": chorus_cnt,\n",
    "        \"verse_count\": vers_cnt,\n",
    "        **sentiment\n",
    "    }\n",
    "    return features\n",
    "\n",
    "\n",
    "def is_file_valid(file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        file_size = os.stat(file_path).st_size\n",
    "        return file_size > 0\n",
    "\n",
    "    return False"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "DATA CURATION UTILS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def remove_duplicates_and_drop_na(df: pd.DataFrame):\n",
    "    clean_df = df.copy()\n",
    "    clean_df.dropna(inplace=True)\n",
    "    clean_df.drop_duplicates([\"name\", \"artists\"],inplace=True)\n",
    "\n",
    "    return clean_df\n",
    "\n",
    "def repair_numeric_missing_vals(df, numeric_cols):\n",
    "    repaired_vals_map = {col: df[col].median() for col in numeric_cols}\n",
    "    repaired_df = df.fillna(value=repaired_vals_map)\n",
    "\n",
    "    return repaired_df\n",
    "\n",
    "def my_dist_to_avg(col):\n",
    "\n",
    "    z_score = (col - col.mean()) / col.std()\n",
    "    col.loc[abs(z_score) > 3] = np.nan\n",
    "\n",
    "def my_iqr(col):\n",
    "    Q1 = col.quantile(0.25)\n",
    "    Q3 = col.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    col.loc[(col < Q1 - 1.5*IQR) | (col > Q3 + 1.5*IQR)] = np.nan\n",
    "\n",
    "def outlier_detection_iqr(df, dfunc):\n",
    "\n",
    "    df_main = df.copy()\n",
    "    [dfunc(df_main[col]) for col in df_main.select_dtypes('number')]\n",
    "\n",
    "    return df_main\n",
    "\n",
    "def transfer_to_categorical(df, numeric_to_bin_value_dict, categorical_col_names):\n",
    "    bin_cols = list(numeric_to_bin_value_dict.keys())\n",
    "    transferred_df = df.copy()\n",
    "    for col in bin_cols:\n",
    "        transferred_df[f\"{col}_categotial\"] = pd.cut(\n",
    "            transferred_df[col],\n",
    "            numeric_to_bin_value_dict[col],\n",
    "            labels=[1,2,3,4,5]\n",
    "        )\n",
    "\n",
    "    transferred_df = pd.get_dummies(\n",
    "        data=transferred_df,\n",
    "        columns=categorical_col_names,\n",
    "        prefix=categorical_col_names\n",
    "    )\n",
    "\n",
    "    return transferred_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "RECALCULATE DATASET"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def recalculate_dataset(dataset_df):\n",
    "    for i, row in dataset_df.iterrows():\n",
    "        genre_list = row[\"genres\"]\n",
    "        # Convert genres back to list\n",
    "        genre_list = ast.literal_eval(genre_list)\n",
    "        recalculate_common_genre = get_common_genre(genre_list)\n",
    "        if recalculate_common_genre != row['common_genre']:\n",
    "            print(f\"common_genre CHANGED FROM {row['common_genre']} TO {recalculate_common_genre}\")\n",
    "            dataset_df.at[i,'common_genre'] = recalculate_common_genre\n",
    "\n",
    "    print(\"DONE\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "PLOT UTILS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def one_dim_plot(sr, plot_type, axis):\n",
    "    if plot_type == 'bar':\n",
    "        axis.bar(sr.index, sr.values)\n",
    "    elif plot_type == 'pie':\n",
    "        axis.pie(sr.values, labels=sr.index, autopct='%1.1f%%')\n",
    "    elif plot_type == 'line':\n",
    "        axis.plot(sr.index, sr.values, marker='o')\n",
    "    else:\n",
    "        print(\"Invalid plot type. Please choose 'bar', 'pie', or 'line'.\")\n",
    "\n",
    "\n",
    "def get_frequent_elements(df, col_name, num_top_elements):\n",
    "    return df[col_name].value_counts().nlargest(num_top_elements).sort_index()\n",
    "\n",
    "\n",
    "def plot_frequent_elements(df, df_params):\n",
    "    fig, axs = plt.subplots(1, len(df_params), figsize=(20, 5))\n",
    "    for i, row in df_params.iterrows():\n",
    "        col_name = row['col_name']\n",
    "        plot_type = row['plot_type']\n",
    "        num_top_elements = row['num_top_elements']\n",
    "        sr = get_frequent_elements(df, col_name, num_top_elements)\n",
    "        one_dim_plot(sr, plot_type, axs[i])\n",
    "        axs[i].set_xlabel(col_name)\n",
    "        axs[i].set_ylabel('Frequency')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "FETCH SONGS DATA USING SPOTIFY API"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset_file = \"dataset.csv\"\n",
    "query = \"year:2000-2023\"\n",
    "dataset = []\n",
    "song_per_genre = 10\n",
    "offset_range = int(song_per_genre/50) or 1\n",
    "print(f\"GOING TO FETCH {song_per_genre*len(GENRE_LIST)} TRACKS\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for genre in GENRE_LIST:\n",
    "    for offset in range(offset_range):\n",
    "        limit = min(50, song_per_genre-50*offset)\n",
    "        try:\n",
    "            print(f\"\\nFETCHING {song_per_genre} {genre.upper()} SONGS, LIMIT: {limit}\")\n",
    "            res = fetch_tracks(query + f\" genre:{genre}\", limit=limit, offset=offset*50)\n",
    "            if not res:\n",
    "                print(f\"FAILED TO FETCH {genre.upper()} SONGS\")\n",
    "                continue\n",
    "\n",
    "            for index, track in enumerate(res):\n",
    "                print(f\"\\nPARSING SONG DATA ({index+1})\")\n",
    "                data = fetch_track_data(track)\n",
    "                dataset.append(data)\n",
    "                print(f\"SONG DATA SAVED\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"FAILED FETCHING TRACKS: {e}\")\n",
    "\n",
    "save_dataset(dataset, dataset_file, True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "PATCH - RECALCULATIONS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = load_dataset(dataset_file)\n",
    "recalculate_dataset(df)\n",
    "save_dataset(df, dataset_file, True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset = load_dataset(dataset_file)\n",
    "print(f\"SHAPE: {dataset.shape}\\n\")\n",
    "dataset.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "CLEAN DATASET"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dup_na_removed = remove_duplicates_and_drop_na(dataset)\n",
    "outliers = outlier_detection_iqr(dup_na_removed, my_dist_to_avg)\n",
    "repaired = repair_numeric_missing_vals(outliers, dup_na_removed.select_dtypes('number'))\n",
    "repaired.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# pd.plotting.scatter_matrix(repaired, figsize = [20,20])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "numeric_to_bin_dict = {\n",
    "    \"duration\": [0, 150000, 300000, 450000, 600000, 750000],\n",
    "    \"popularity\": [0, 20, 40, 60, 80, 100],\n",
    "    \"lines_count\": [0, 50, 100, 150, 200, 250],\n",
    "    \"word_count\": [0, 300, 600, 900, 1200, 1500],\n",
    "    \"unique_words\": [0, 100, 200, 300, 400, 500],\n",
    "    \"stopwords_count\": [-1, 150, 300, 450, 600, 750],\n",
    "}\n",
    "\n",
    "categorical_cols = ['chorus_count', 'verse_count']\n",
    "transferred = transfer_to_categorical(repaired, numeric_to_bin_dict, categorical_cols)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "transferred.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "EDA SECTION"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_params = pd.DataFrame({'plot_type': ['bar', 'line', 'pie'],\n",
    "                          'col_name': ['common_genre', 'release_date', 'artists'],\n",
    "                          'num_top_elements': [6,6,6]})\n",
    "plot_frequent_elements(transferred, df_params)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (18,6)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"\\nCommon Genre Distribution:\")\n",
    "sns.countplot(x='common_genre', data=dataset)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "numerical_cols = ['duration','popularity', 'lines_count', 'word_count', 'unique_words',\n",
    "                  'stopwords_count', 'avg_word_length', 'chorus_count', 'verse_count']\n",
    "for col in numerical_cols:\n",
    "    sns.histplot(dataset[col])\n",
    "    plt.title(col + \" Distribution\")\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sentiment_cols = ['neg', 'neu', 'pos', 'compound']\n",
    "for col in sentiment_cols:\n",
    "    sns.histplot(dataset[col])\n",
    "    plt.title(col + \" Distribution\")\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
